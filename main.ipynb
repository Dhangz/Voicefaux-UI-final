{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cebfe8",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Audio Preprocessing and Denoising\n",
    "# Install required packages first:\n",
    "# pip install librosa pandas numpy noisereduce soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197daad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BASIC SYSTEM & UTILITIES\n",
    "# =========================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# NUMERICAL & DATA HANDLING\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# =========================\n",
    "# AUDIO PROCESSING\n",
    "# =========================\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "\n",
    "# =========================\n",
    "# VISUALIZATION\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# SCIKIT-LEARN (CLASSICAL ML)\n",
    "# =========================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# =========================\n",
    "# XGBOOST\n",
    "# =========================\n",
    "import xgboost as xgb\n",
    "\n",
    "# =========================\n",
    "# PYTORCH CORE\n",
    "# =========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# =========================\n",
    "# TORCHVISION & MODELS\n",
    "# =========================\n",
    "from torchvision import transforms\n",
    "\n",
    "import torchvision.models as tv_models\n",
    "\n",
    "import timm\n",
    "\n",
    "# =========================\n",
    "# OTHER\n",
    "# =========================\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2bdee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing modified...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:16<00:00, 18.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing unmodified...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:17<00:00, 16.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing synthetic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:16<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing spliced...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:20<00:00, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Preprocessing Complete\n",
      "MFCC shape: (1200, 157, 40)\n",
      "Class distribution:\n",
      "label\n",
      "0    300\n",
      "1    300\n",
      "2    300\n",
      "3    300\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "DATASET_DIR = \"datasets\"\n",
    "OUTPUT_DIR = \"datasets_cleaned\"\n",
    "SR = 16000\n",
    "DURATION = 5          # seconds\n",
    "SAMPLES = SR * DURATION\n",
    "N_MFCC = 40\n",
    "\n",
    "CLASSES = {\n",
    "    \"modified\": 0,\n",
    "    \"unmodified\": 1,\n",
    "    \"synthetic\": 2,\n",
    "    \"spliced\": 3\n",
    "}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# AUDIO FUNCTIONS\n",
    "# =========================\n",
    "def load_audio(path):\n",
    "    audio, _ = librosa.load(path, sr=SR, mono=True)\n",
    "    audio = nr.reduce_noise(y=audio, sr=SR)\n",
    "    return audio\n",
    "\n",
    "def fix_length(audio):\n",
    "    if len(audio) < SAMPLES:\n",
    "        return np.pad(audio, (0, SAMPLES - len(audio)))\n",
    "    return audio[:SAMPLES]\n",
    "\n",
    "def extract_mfcc(audio):\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio,\n",
    "        sr=SR,\n",
    "        n_mfcc=N_MFCC\n",
    "    )\n",
    "    return mfcc.T  # (time, features)\n",
    "\n",
    "# =========================\n",
    "# PROCESS DATASET\n",
    "# =========================\n",
    "X, y, filenames = [], [], []\n",
    "\n",
    "for class_name, label in CLASSES.items():\n",
    "    class_path = os.path.join(DATASET_DIR, class_name)\n",
    "    output_class = os.path.join(OUTPUT_DIR, class_name)\n",
    "    os.makedirs(output_class, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nProcessing {class_name}...\")\n",
    "\n",
    "    for file in tqdm(os.listdir(class_path)):\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(class_path, file)\n",
    "\n",
    "        try:\n",
    "            audio = load_audio(path)\n",
    "            audio = fix_length(audio)\n",
    "\n",
    "            # Save clean audio\n",
    "            sf.write(os.path.join(output_class, file), audio, SR)\n",
    "\n",
    "            mfcc = extract_mfcc(audio)\n",
    "\n",
    "            X.append(mfcc)\n",
    "            y.append(label)\n",
    "            filenames.append(file)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {file} - {e}\")\n",
    "\n",
    "# =========================\n",
    "# SAVE DATA\n",
    "# =========================\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "np.save(\"X_mfcc.npy\", X)\n",
    "np.save(\"y_labels.npy\", y)\n",
    "\n",
    "metadata = pd.DataFrame({\n",
    "    \"filename\": filenames,\n",
    "    \"label\": y\n",
    "})\n",
    "metadata.to_csv(\"metadata.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing Complete\")\n",
    "print(\"MFCC shape:\", X.shape)\n",
    "print(\"Class distribution:\")\n",
    "print(metadata[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a48112",
   "metadata": {},
   "source": [
    "# Step 2: Feature Extraction - MFCCs and Mel-Spectrograms\n",
    "# This creates both MFCC features and Mel-Spectrogram images for ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff2aaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting MFCCs and Mel-Spectrograms...\n",
      "\n",
      "Processing modified...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:02<00:00, 124.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing unmodified...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:02<00:00, 120.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing synthetic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:02<00:00, 116.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spliced...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:02<00:00, 112.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì STEP 2 COMPLETE\n",
      "Processed: 1200\n",
      "Failed: 0\n",
      "MFCC shape: (39, 216)\n",
      "Mel-Spectrogram shape: (128, 216, 3)\n",
      "Labels saved to labels_step2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 2: FEATURE EXTRACTION\n",
    "# MFCCs + Mel-Spectrograms\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION (ALIGNED WITH STEP 1)\n",
    "# =========================\n",
    "CLEAN_AUDIO_FOLDER = \"datasets_cleaned\"\n",
    "MFCC_OUTPUT_FOLDER = \"mfcc_features\"\n",
    "MELSPEC_OUTPUT_FOLDER = \"melspec_features\"\n",
    "\n",
    "SR = 16000\n",
    "DURATION = 5                      # seconds\n",
    "SAMPLES = SR * DURATION\n",
    "N_MFCC = 13\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 512\n",
    "FIXED_FRAMES = 216                # consistent time dimension\n",
    "\n",
    "CLASSES = {\n",
    "    \"modified\": 0,\n",
    "    \"unmodified\": 1,\n",
    "    \"synthetic\": 2,\n",
    "    \"spliced\": 3\n",
    "}\n",
    "\n",
    "os.makedirs(MFCC_OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(MELSPEC_OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# FUNCTIONS\n",
    "# =========================\n",
    "def load_audio_fixed(path):\n",
    "    y, _ = librosa.load(path, sr=SR, mono=True)\n",
    "    y = librosa.util.fix_length(y, size=SAMPLES)\n",
    "    return y\n",
    "\n",
    "def extract_mfcc(audio):\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio,\n",
    "        sr=SR,\n",
    "        n_mfcc=N_MFCC,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH\n",
    "    )\n",
    "\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "    mfcc = np.vstack([mfcc, delta, delta2])\n",
    "    mfcc = librosa.util.fix_length(mfcc, size=FIXED_FRAMES, axis=1)\n",
    "\n",
    "    return mfcc   # (39, 216)\n",
    "\n",
    "def extract_melspec(audio):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=SR,\n",
    "        n_mels=N_MELS,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH\n",
    "    )\n",
    "\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    mel_db = librosa.util.fix_length(mel_db, size=FIXED_FRAMES, axis=1)\n",
    "\n",
    "    # Convert to 3-channel for ResNet\n",
    "    mel_rgb = np.stack([mel_db] * 3, axis=-1)\n",
    "\n",
    "    return mel_rgb   # (128, 216, 3)\n",
    "\n",
    "# =========================\n",
    "# PROCESS DATASET\n",
    "# =========================\n",
    "files, labels = [], []\n",
    "processed, failed = 0, 0\n",
    "\n",
    "print(\"\\nExtracting MFCCs and Mel-Spectrograms...\\n\")\n",
    "\n",
    "for class_name, label in CLASSES.items():\n",
    "    class_path = os.path.join(CLEAN_AUDIO_FOLDER, class_name)\n",
    "\n",
    "    if not os.path.exists(class_path):\n",
    "        print(f\"Skipping missing folder: {class_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {class_name}...\")\n",
    "\n",
    "    for file in tqdm(os.listdir(class_path)):\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(class_path, file)\n",
    "        file_id = f\"{class_name}_{os.path.splitext(file)[0]}\"\n",
    "\n",
    "        try:\n",
    "            audio = load_audio_fixed(file_path)\n",
    "\n",
    "            mfcc = extract_mfcc(audio)\n",
    "            mel = extract_melspec(audio)\n",
    "\n",
    "            np.save(os.path.join(MFCC_OUTPUT_FOLDER, file_id + \".npy\"), mfcc)\n",
    "            np.save(os.path.join(MELSPEC_OUTPUT_FOLDER, file_id + \".npy\"), mel)\n",
    "\n",
    "            files.append(file_id)\n",
    "            labels.append(label)\n",
    "            processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {file} -> {e}\")\n",
    "            failed += 1\n",
    "\n",
    "# =========================\n",
    "# SAVE LABEL FILE\n",
    "# =========================\n",
    "df = pd.DataFrame({\n",
    "    \"file\": files,\n",
    "    \"label\": labels\n",
    "})\n",
    "df.to_csv(\"labels_step2.csv\", index=False)\n",
    "\n",
    "# =========================\n",
    "# SUMMARY\n",
    "# =========================\n",
    "print(\"\\n‚úì STEP 2 COMPLETE\")\n",
    "print(f\"Processed: {processed}\")\n",
    "print(f\"Failed: {failed}\")\n",
    "print(\"MFCC shape:\", mfcc.shape)\n",
    "print(\"Mel-Spectrogram shape:\", mel.shape)\n",
    "print(\"Labels saved to labels_step2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883507eb",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Dataset Preparation for 4-Class Classification\n",
    "# This prepares data for both traditional ML models and ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d8aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1200\n",
      "\n",
      "Loading MFCC features...\n",
      "MFCC shape: (1200, 39, 216)\n",
      "\n",
      "Class Distribution:\n",
      "Class 0: 300\n",
      "Class 1: 300\n",
      "Class 2: 300\n",
      "Class 3: 300\n",
      "\n",
      "Dataset Split:\n",
      "Train: 840\n",
      "Val:   120\n",
      "Test:  240\n",
      "\n",
      "‚úì MFCC DATA READY FOR ML MODELS\n",
      "\n",
      "Preparing Mel-Spectrogram data for ResNet...\n",
      "Mel-Spectrogram shape: (1200, 128, 216, 3)\n",
      "\n",
      "‚úì STEP 3 COMPLETE\n",
      "‚úì MFCC ‚Üí Traditional ML\n",
      "‚úì Mel-Spectrogram ‚Üí ResNet\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 3: DATASET PREPARATION\n",
    "# 4-CLASS CLASSIFICATION\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION (ALIGNED)\n",
    "# =========================\n",
    "MFCC_FOLDER = \"mfcc_features\"\n",
    "MELSPEC_FOLDER = \"melspec_features\"   # NPYS, NOT IMAGES\n",
    "LABEL_FILE = \"labels_step2.csv\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =========================\n",
    "# LOAD LABELS\n",
    "# =========================\n",
    "labels_df = pd.read_csv(LABEL_FILE)\n",
    "\n",
    "files = labels_df[\"file\"].values\n",
    "labels = labels_df[\"label\"].values\n",
    "\n",
    "print(f\"Total samples: {len(labels)}\")\n",
    "\n",
    "# =========================\n",
    "# LOAD MFCC FEATURES\n",
    "# =========================\n",
    "print(\"\\nLoading MFCC features...\")\n",
    "\n",
    "X_mfcc = []\n",
    "valid_files = []\n",
    "valid_labels = []\n",
    "\n",
    "for file_id, label in zip(files, labels):\n",
    "    path = os.path.join(MFCC_FOLDER, file_id + \".npy\")\n",
    "    if os.path.exists(path):\n",
    "        mfcc = np.load(path)\n",
    "        X_mfcc.append(mfcc)\n",
    "        valid_files.append(file_id)\n",
    "        valid_labels.append(label)\n",
    "\n",
    "X_mfcc = np.array(X_mfcc)\n",
    "y = np.array(valid_labels)\n",
    "\n",
    "print(\"MFCC shape:\", X_mfcc.shape)  # (samples, 39, 216)\n",
    "\n",
    "# =========================\n",
    "# CLASS DISTRIBUTION\n",
    "# =========================\n",
    "print(\"\\nClass Distribution:\")\n",
    "for k, v in Counter(y).items():\n",
    "    print(f\"Class {k}: {v}\")\n",
    "\n",
    "# =========================\n",
    "# PREPARE MFCC FOR ML\n",
    "# =========================\n",
    "samples = X_mfcc.shape[0]\n",
    "X_mfcc_flat = X_mfcc.reshape(samples, -1)\n",
    "\n",
    "# =========================\n",
    "# SPLIT DATA (STRATIFIED)\n",
    "# =========================\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
    "    X_mfcc_flat, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_temp, y_train_temp,\n",
    "    test_size=VAL_SIZE / (1 - TEST_SIZE),\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train_temp\n",
    ")\n",
    "\n",
    "print(\"\\nDataset Split:\")\n",
    "print(f\"Train: {len(X_train)}\")\n",
    "print(f\"Val:   {len(X_val)}\")\n",
    "print(f\"Test:  {len(X_test)}\")\n",
    "\n",
    "# =========================\n",
    "# SCALE FEATURES\n",
    "# =========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =========================\n",
    "# SAVE ML DATA\n",
    "# =========================\n",
    "np.save(\"X_train_mfcc.npy\", X_train_scaled)\n",
    "np.save(\"X_val_mfcc.npy\", X_val_scaled)\n",
    "np.save(\"X_test_mfcc.npy\", X_test_scaled)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "with open(\"mfcc_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"\\n‚úì MFCC DATA READY FOR ML MODELS\")\n",
    "\n",
    "# ======================================================\n",
    "# PREPARE MEL-SPECTROGRAM DATA FOR RESNET\n",
    "# ======================================================\n",
    "print(\"\\nPreparing Mel-Spectrogram data for ResNet...\")\n",
    "\n",
    "X_mel = []\n",
    "\n",
    "for file_id in valid_files:\n",
    "    path = os.path.join(MELSPEC_FOLDER, file_id + \".npy\")\n",
    "    if os.path.exists(path):\n",
    "        mel = np.load(path)\n",
    "        X_mel.append(mel)\n",
    "\n",
    "X_mel = np.array(X_mel)\n",
    "print(\"Mel-Spectrogram shape:\", X_mel.shape)  # (samples, 128, 216, 3)\n",
    "\n",
    "# SAME SPLITS (important!)\n",
    "X_mel_train = X_mel[:len(y_train)]\n",
    "X_mel_val = X_mel[len(y_train):len(y_train)+len(y_val)]\n",
    "X_mel_test = X_mel[-len(y_test):]\n",
    "\n",
    "np.save(\"X_train_mel.npy\", X_mel_train)\n",
    "np.save(\"X_val_mel.npy\", X_mel_val)\n",
    "np.save(\"X_test_mel.npy\", X_mel_test)\n",
    "\n",
    "print(\"\\n‚úì STEP 3 COMPLETE\")\n",
    "print(\"‚úì MFCC ‚Üí Traditional ML\")\n",
    "print(\"‚úì Mel-Spectrogram ‚Üí ResNet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeee307",
   "metadata": {},
   "source": [
    "# Step 4: Training Traditional ML Models for 4-Class Classification\n",
    "# pip install xgboost scikit-learn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54015c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed MFCC data...\n",
      "Data loaded successfully\n",
      "Train shape: (840, 8424)\n",
      "Test shape: (240, 8424)\n",
      "\n",
      "============================================================\n",
      "Training XGBoost\n",
      "============================================================\n",
      "Train Acc: 1.0000\n",
      "Val   Acc: 0.8167\n",
      "Test  Acc: 0.8000\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    modified     0.7258    0.7500    0.7377        60\n",
      "  unmodified     0.7119    0.7000    0.7059        60\n",
      "   synthetic     0.8983    0.8833    0.8908        60\n",
      "     spliced     0.8667    0.8667    0.8667        60\n",
      "\n",
      "    accuracy                         0.8000       240\n",
      "   macro avg     0.8007    0.8000    0.8003       240\n",
      "weighted avg     0.8007    0.8000    0.8003       240\n",
      "\n",
      "Model saved: XGBoost_model.pkl\n",
      "\n",
      "============================================================\n",
      "Training RandomForest\n",
      "============================================================\n",
      "Train Acc: 1.0000\n",
      "Val   Acc: 0.7917\n",
      "Test  Acc: 0.7125\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    modified     0.6508    0.6833    0.6667        60\n",
      "  unmodified     0.6316    0.6000    0.6154        60\n",
      "   synthetic     0.8750    0.7000    0.7778        60\n",
      "     spliced     0.7222    0.8667    0.7879        60\n",
      "\n",
      "    accuracy                         0.7125       240\n",
      "   macro avg     0.7199    0.7125    0.7119       240\n",
      "weighted avg     0.7199    0.7125    0.7119       240\n",
      "\n",
      "Model saved: RandomForest_model.pkl\n",
      "\n",
      "============================================================\n",
      "Training KNN\n",
      "============================================================\n",
      "Train Acc: 0.6214\n",
      "Val   Acc: 0.4833\n",
      "Test  Acc: 0.4667\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    modified     0.3615    0.7833    0.4947        60\n",
      "  unmodified     0.5000    0.2500    0.3333        60\n",
      "   synthetic     1.0000    0.3167    0.4810        60\n",
      "     spliced     0.5082    0.5167    0.5124        60\n",
      "\n",
      "    accuracy                         0.4667       240\n",
      "   macro avg     0.5924    0.4667    0.4554       240\n",
      "weighted avg     0.5924    0.4667    0.4554       240\n",
      "\n",
      "Model saved: KNN_model.pkl\n",
      "\n",
      "============================================================\n",
      "Training LDA\n",
      "============================================================\n",
      "Train Acc: 0.9976\n",
      "Val   Acc: 0.3833\n",
      "Test  Acc: 0.3583\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    modified     0.2976    0.4167    0.3472        60\n",
      "  unmodified     0.3333    0.5333    0.4103        60\n",
      "   synthetic     0.7826    0.3000    0.4337        60\n",
      "     spliced     0.2973    0.1833    0.2268        60\n",
      "\n",
      "    accuracy                         0.3583       240\n",
      "   macro avg     0.4277    0.3583    0.3545       240\n",
      "weighted avg     0.4277    0.3583    0.3545       240\n",
      "\n",
      "Model saved: LDA_model.pkl\n",
      "\n",
      "============================================================\n",
      "Training LogisticRegression\n",
      "============================================================\n",
      "Train Acc: 0.9976\n",
      "Val   Acc: 0.7667\n",
      "Test  Acc: 0.7125\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    modified     0.5467    0.6833    0.6074        60\n",
      "  unmodified     0.5926    0.5333    0.5614        60\n",
      "   synthetic     0.9000    0.9000    0.9000        60\n",
      "     spliced     0.8627    0.7333    0.7928        60\n",
      "\n",
      "    accuracy                         0.7125       240\n",
      "   macro avg     0.7255    0.7125    0.7154       240\n",
      "weighted avg     0.7255    0.7125    0.7154       240\n",
      "\n",
      "Model saved: LogisticRegression_model.pkl\n",
      "\n",
      "============================================================\n",
      "Training SVM\n",
      "============================================================\n",
      "Train Acc: 0.9786\n",
      "Val   Acc: 0.7500\n",
      "Test  Acc: 0.6958\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    modified     0.6029    0.6833    0.6406        60\n",
      "  unmodified     0.5741    0.5167    0.5439        60\n",
      "   synthetic     0.8929    0.8333    0.8621        60\n",
      "     spliced     0.7258    0.7500    0.7377        60\n",
      "\n",
      "    accuracy                         0.6958       240\n",
      "   macro avg     0.6989    0.6958    0.6961       240\n",
      "weighted avg     0.6989    0.6958    0.6961       240\n",
      "\n",
      "Model saved: SVM_model.pkl\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "Model                Test Acc   Test F1   \n",
      "---------------------------------------------\n",
      "XGBoost              0.8000     0.8003    \n",
      "RandomForest         0.7125     0.7119    \n",
      "KNN                  0.4667     0.4554    \n",
      "LDA                  0.3583     0.3545    \n",
      "LogisticRegression   0.7125     0.7154    \n",
      "SVM                  0.6958     0.6961    \n",
      "\n",
      "üèÜ Best Model: XGBoost\n",
      "Test Accuracy: 0.8000\n",
      "Test F1-Score: 0.8003\n",
      "\n",
      "‚úì STEP 4 COMPLETE\n",
      "‚úì Traditional ML models trained and evaluated\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 4: TRAINING TRADITIONAL ML MODELS\n",
    "# 4-CLASS AUDIO CLASSIFICATION\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA (ALIGNED WITH STEP 3)\n",
    "# =========================\n",
    "print(\"Loading preprocessed MFCC data...\")\n",
    "\n",
    "X_train = np.load(\"X_train_mfcc.npy\")\n",
    "X_val   = np.load(\"X_val_mfcc.npy\")\n",
    "X_test  = np.load(\"X_test_mfcc.npy\")\n",
    "\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val   = np.load(\"y_val.npy\")\n",
    "y_test  = np.load(\"y_test.npy\")\n",
    "\n",
    "CLASS_NAMES = [\"modified\", \"unmodified\", \"synthetic\", \"spliced\"]\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "\n",
    "# =========================\n",
    "# DEFINE MODELS\n",
    "# =========================\n",
    "models = {\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=4,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric=\"mlogloss\"\n",
    "    ),\n",
    "\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"KNN\": KNeighborsClassifier(\n",
    "        n_neighbors=5,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=42\n",
    "),\n",
    "\n",
    "\n",
    "    \"SVM\": SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=1.0,\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# TRAIN & EVALUATE\n",
    "# =========================\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred   = model.predict(X_val)\n",
    "    y_test_pred  = model.predict(X_test)\n",
    "\n",
    "    results[name] = {\n",
    "        \"train_acc\": accuracy_score(y_train, y_train_pred),\n",
    "        \"val_acc\": accuracy_score(y_val, y_val_pred),\n",
    "        \"test_acc\": accuracy_score(y_test, y_test_pred),\n",
    "        \"train_f1\": f1_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "        \"val_f1\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
    "        \"test_f1\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "    print(f\"Train Acc: {results[name]['train_acc']:.4f}\")\n",
    "    print(f\"Val   Acc: {results[name]['val_acc']:.4f}\")\n",
    "    print(f\"Test  Acc: {results[name]['test_acc']:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report (Test):\")\n",
    "    print(classification_report(\n",
    "        y_test, y_test_pred,\n",
    "        target_names=CLASS_NAMES,\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "    # =========================\n",
    "    # CONFUSION MATRIX\n",
    "    # =========================\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm)\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(range(4), CLASS_NAMES, rotation=45)\n",
    "    plt.yticks(range(4), CLASS_NAMES)\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{name}_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # =========================\n",
    "    # SAVE MODEL\n",
    "    # =========================\n",
    "    with open(f\"{name}_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"Model saved: {name}_model.pkl\")\n",
    "\n",
    "# =========================\n",
    "# MODEL COMPARISON SUMMARY\n",
    "# =========================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"{'Model':<20} {'Test Acc':<10} {'Test F1':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for name, m in results.items():\n",
    "    print(f\"{name:<20} {m['test_acc']:<10.4f} {m['test_f1']:<10.4f}\")\n",
    "\n",
    "best_model = max(results, key=lambda x: results[x][\"test_acc\"])\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"Test Accuracy: {results[best_model]['test_acc']:.4f}\")\n",
    "print(f\"Test F1-Score: {results[best_model]['test_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì STEP 4 COMPLETE\")\n",
    "print(\"‚úì Traditional ML models trained and evaluated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7bd13",
   "metadata": {},
   "source": [
    "\n",
    "# Step 5: Prepare Dataset for ResNet with Mel-Spectrograms\n",
    "# pip install torch torchvision pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c34e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1200\n",
      "\n",
      "Label Distribution:\n",
      "Class 0: 300\n",
      "Class 1: 300\n",
      "Class 2: 300\n",
      "Class 3: 300\n",
      "\n",
      "Dataset Split:\n",
      "Train: 840\n",
      "Val:   120\n",
      "Test:  240\n",
      "\n",
      "Sample Batch Check:\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Labels: tensor([0, 3, 0, 3, 2])\n",
      "\n",
      "‚úì STEP 5 COMPLETE\n",
      "‚úì Mel-Spectrogram dataset ready for ResNet\n",
      "‚úì Fully aligned with Steps 1‚Äì4\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 5: PREPARE DATASET FOR RESNET\n",
    "# (ALIGNED WITH STEPS 1‚Äì4)\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION (ALIGNED)\n",
    "# =========================\n",
    "MELSPEC_FOLDER = \"melspec_features\"   # .npy files\n",
    "LABEL_FILE = \"labels_step2.csv\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =========================\n",
    "# LOAD LABELS\n",
    "# =========================\n",
    "labels_df = pd.read_csv(LABEL_FILE)\n",
    "\n",
    "files = labels_df[\"file\"].values\n",
    "labels = labels_df[\"label\"].values\n",
    "\n",
    "print(f\"Total samples: {len(labels)}\")\n",
    "\n",
    "print(\"\\nLabel Distribution:\")\n",
    "for k, v in Counter(labels).items():\n",
    "    print(f\"Class {k}: {v}\")\n",
    "\n",
    "# =========================\n",
    "# TRAIN / VAL / TEST SPLIT\n",
    "# =========================\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    files, labels,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=VAL_SIZE / (1 - TEST_SIZE),\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"\\nDataset Split:\")\n",
    "print(f\"Train: {len(X_train)}\")\n",
    "print(f\"Val:   {len(X_val)}\")\n",
    "print(f\"Test:  {len(X_test)}\")\n",
    "\n",
    "# =========================\n",
    "# DATASET CLASS\n",
    "# =========================\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_ids, labels, transform=None):\n",
    "        self.file_ids = file_ids\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.file_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        mel = np.load(os.path.join(MELSPEC_FOLDER, file_id + \".npy\"))\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).permute(2, 0, 1)  # (C,H,W)\n",
    "\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "\n",
    "        return mel, label\n",
    "\n",
    "# =========================\n",
    "# TRANSFORMS (RESNET)\n",
    "# =========================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# DATA LOADERS\n",
    "# =========================\n",
    "train_dataset = MelSpectrogramDataset(X_train, y_train, train_transform)\n",
    "val_dataset   = MelSpectrogramDataset(X_val, y_val, val_test_transform)\n",
    "test_dataset  = MelSpectrogramDataset(X_test, y_test, val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# =========================\n",
    "# SANITY CHECK\n",
    "# =========================\n",
    "sample_x, sample_y = next(iter(train_loader))\n",
    "\n",
    "print(\"\\nSample Batch Check:\")\n",
    "print(\"Input shape:\", sample_x.shape)   # (B, 3, 224, 224)\n",
    "print(\"Labels:\", sample_y[:5])\n",
    "\n",
    "# =========================\n",
    "# SAVE DATASET INFO\n",
    "# =========================\n",
    "dataset_info = {\n",
    "    \"train_files\": X_train.tolist(),\n",
    "    \"val_files\": X_val.tolist(),\n",
    "    \"test_files\": X_test.tolist(),\n",
    "    \"num_classes\": 4,\n",
    "    \"class_names\": [\"modified\", \"unmodified\", \"synthetic\", \"spliced\"]\n",
    "}\n",
    "\n",
    "with open(\"resnet_dataset_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset_info, f)\n",
    "\n",
    "print(\"\\n‚úì STEP 5 COMPLETE\")\n",
    "print(\"‚úì Mel-Spectrogram dataset ready for ResNet\")\n",
    "print(\"‚úì Fully aligned with Steps 1‚Äì4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e9f04",
   "metadata": {},
   "source": [
    "# Step 6: Train ResNet and EfficientNet Models\n",
    "# pip install torch torchvision timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a93745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training resnet50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'resnet50'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 230\u001b[39m\n\u001b[32m    227\u001b[39m results = {}\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, cfg \u001b[38;5;129;01min\u001b[39;00m experiments.items():\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     acc, f1 = evaluate(model, model_name)\n\u001b[32m    234\u001b[39m     torch.save(model.state_dict(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(backbone, epochs, lr)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model\u001b[39m(backbone, epochs=\u001b[32m25\u001b[39m, lr=\u001b[32m1e-3\u001b[39m):\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     model = \u001b[43mAudioClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m    158\u001b[39m     criterion = nn.CrossEntropyLoss()\n\u001b[32m    159\u001b[39m     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=\u001b[32m1e-4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mAudioClassifier.__init__\u001b[39m\u001b[34m(self, backbone)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backbone == \u001b[33m\"\u001b[39m\u001b[33mresnet50\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresnet50\u001b[49m(weights=\u001b[33m\"\u001b[39m\u001b[33mIMAGENET1K_V1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.fc = nn.Linear(\u001b[38;5;28mself\u001b[39m.model.fc.in_features, NUM_CLASSES)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backbone == \u001b[33m\"\u001b[39m\u001b[33mefficientnet_b0\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'resnet50'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 6: TRAIN RESNET & EFFICIENTNET (GPU ENABLED)\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# GPU SETTINGS\n",
    "# =========================\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA INFO\n",
    "# =========================\n",
    "with open(\"resnet_dataset_info.pkl\", \"rb\") as f:\n",
    "    dataset_info = pickle.load(f)\n",
    "\n",
    "labels_df = pd.read_csv(\"labels_step2.csv\")\n",
    "label_map = dict(zip(labels_df[\"file\"], labels_df[\"label\"]))\n",
    "\n",
    "CLASS_NAMES = [\"modified\", \"unmodified\", \"synthetic\", \"spliced\"]\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# =========================\n",
    "# DATASET CLASS\n",
    "# =========================\n",
    "class MelDataset(Dataset):\n",
    "    def __init__(self, file_ids, labels, transform=None):\n",
    "        self.file_ids = file_ids\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = np.load(f\"melspec_features/{self.file_ids[idx]}.npy\")\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "\n",
    "        return mel, self.labels[idx]\n",
    "\n",
    "# =========================\n",
    "# TRANSFORMS\n",
    "# =========================\n",
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64  # reduce to 32 if GPU memory is low\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# DATASETS & LOADERS\n",
    "# =========================\n",
    "train_ds = MelDataset(\n",
    "    dataset_info[\"train_files\"],\n",
    "    [label_map[f] for f in dataset_info[\"train_files\"]],\n",
    "    transform\n",
    ")\n",
    "\n",
    "val_ds = MelDataset(\n",
    "    dataset_info[\"val_files\"],\n",
    "    [label_map[f] for f in dataset_info[\"val_files\"]],\n",
    "    transform\n",
    ")\n",
    "\n",
    "test_ds = MelDataset(\n",
    "    dataset_info[\"test_files\"],\n",
    "    [label_map[f] for f in dataset_info[\"test_files\"]],\n",
    "    transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# MODEL WRAPPER\n",
    "# =========================\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet50\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if backbone == \"resnet50\":\n",
    "            self.model = tv_models.resnet50(\n",
    "                weights=tv_models.ResNet50_Weights.IMAGENET1K_V1\n",
    "            )\n",
    "            self.model.fc = nn.Linear(\n",
    "                self.model.fc.in_features,\n",
    "                NUM_CLASSES\n",
    "            )\n",
    "\n",
    "        elif backbone == \"efficientnet_b0\":\n",
    "            self.model = timm.create_model(\n",
    "                \"efficientnet_b0\",\n",
    "                pretrained=True,\n",
    "                num_classes=NUM_CLASSES\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TRAIN / VALIDATE LOOP\n",
    "# =========================\n",
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    training = optimizer is not None\n",
    "    model.train() if training else model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    preds, labels_all = [], []\n",
    "\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        labels_all.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    f1 = f1_score(labels_all, preds, average=\"weighted\")\n",
    "\n",
    "    return acc, f1, preds, labels_all\n",
    "\n",
    "# =========================\n",
    "# TRAIN MODEL\n",
    "# =========================\n",
    "def train_model(backbone, epochs=25, lr=1e-3):\n",
    "    print(f\"\\nTraining {backbone}\")\n",
    "\n",
    "    model = AudioClassifier(backbone).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"max\", patience=3)\n",
    "\n",
    "    best_acc = 0\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        train_acc, _, _, _ = run_epoch(\n",
    "            model, train_loader, criterion, optimizer\n",
    "        )\n",
    "\n",
    "        val_acc, val_f1, _, _ = run_epoch(\n",
    "            model, val_loader, criterion\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(f\"Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Acc: {val_acc:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "def evaluate(model, name):\n",
    "    acc, f1, preds, labels = run_epoch(\n",
    "        model, test_loader, nn.CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{name} TEST RESULTS\")\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(classification_report(labels, preds, target_names=CLASS_NAMES))\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.imshow(cm)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xticks(range(4), CLASS_NAMES, rotation=45)\n",
    "    plt.yticks(range(4), CLASS_NAMES)\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{name}_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return acc, f1\n",
    "\n",
    "# =========================\n",
    "# RUN EXPERIMENTS\n",
    "# =========================\n",
    "\n",
    "\n",
    "experiments = {\n",
    "    \"resnet50\": {\"epochs\": 25, \"lr\": 1e-3},\n",
    "    \"efficientnet_b0\": {\"epochs\": 25, \"lr\": 1e-3}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, cfg in experiments.items():\n",
    "    model = train_model(model_name, cfg[\"epochs\"], cfg[\"lr\"])\n",
    "\n",
    "    acc, f1 = evaluate(model, model_name)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
    "    results[model_name] = {\"acc\": acc, \"f1\": f1}\n",
    "\n",
    "# =========================\n",
    "# SUMMARY\n",
    "# =========================\n",
    "print(\"\\nFINAL RESULTS\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name}: Accuracy={res['acc']:.2f}%, F1={res['f1']:.4f}\")\n",
    "\n",
    "best_model = max(results, key=lambda x: results[x][\"acc\"])\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044d4d5",
   "metadata": {},
   "source": [
    "\n",
    "# Step 7: Complete Inference Pipeline for New Audio Files\n",
    "# This allows you to predict on new audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 7: INFERENCE PIPELINE (ALIGNED)\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION (SAME AS TRAINING)\n",
    "# =========================\n",
    "SR = 16000\n",
    "DURATION = 5\n",
    "SAMPLES = SR * DURATION\n",
    "IMG_SIZE = 224\n",
    "\n",
    "CLASS_NAMES = [\"modified\", \"unmodified\", \"synthetic\", \"spliced\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================\n",
    "# MODEL DEFINITION\n",
    "# =========================\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet50\", num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        if backbone == \"resnet50\":\n",
    "            self.model = models.resnet50(weights=None)\n",
    "            self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "        elif backbone == \"efficientnet_b0\":\n",
    "            self.model = timm.create_model(\n",
    "                \"efficientnet_b0\",\n",
    "                pretrained=False,\n",
    "                num_classes=num_classes\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# =========================\n",
    "# AUDIO PREPROCESSING\n",
    "# =========================\n",
    "def preprocess_audio(audio_path):\n",
    "    y, _ = librosa.load(audio_path, sr=SR, mono=True)\n",
    "    y = librosa.util.fix_length(y, size=SAMPLES)\n",
    "    y = nr.reduce_noise(y=y, sr=SR)\n",
    "    return y\n",
    "\n",
    "def create_melspectrogram(y):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y, sr=SR, n_mels=128, n_fft=1024, hop_length=512\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    mel_db = librosa.util.fix_length(mel_db, size=216, axis=1)\n",
    "    mel_rgb = np.stack([mel_db] * 3, axis=-1)  # (H, W, 3)\n",
    "    return mel_rgb\n",
    "\n",
    "# =========================\n",
    "# TRANSFORM (SAME AS TRAINING)\n",
    "# =========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# PREDICTOR CLASS\n",
    "# =========================\n",
    "class AudioPredictor:\n",
    "    def __init__(self, model_name=\"resnet50\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = AudioClassifier(model_name).to(device)\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(f\"{model_name}.pth\", map_location=device)\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, audio_path):\n",
    "        y = preprocess_audio(audio_path)\n",
    "        mel = create_melspectrogram(y)\n",
    "        x = transform(mel).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(x)\n",
    "            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "            pred = int(np.argmax(probs))\n",
    "\n",
    "        return {\n",
    "            \"predicted_class\": CLASS_NAMES[pred],\n",
    "            \"confidence\": float(probs[pred]),\n",
    "            \"all_probabilities\": dict(zip(CLASS_NAMES, probs))\n",
    "        }\n",
    "\n",
    "# =========================\n",
    "# VISUALIZATION (OPTIONAL)\n",
    "# =========================\n",
    "def visualize(audio_path, result):\n",
    "    y = preprocess_audio(audio_path)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=SR)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_db, sr=SR, x_axis=\"time\", y_axis=\"mel\")\n",
    "    plt.title(\n",
    "        f\"Prediction: {result['predicted_class']} \"\n",
    "        f\"({result['confidence']*100:.2f}%)\"\n",
    "    )\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =========================\n",
    "# EXAMPLE USAGE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = AudioPredictor(\"resnet50\")  # or \"efficientnet_b0\"\n",
    "\n",
    "    audio_file = \"your_audio.wav\"\n",
    "    if os.path.exists(audio_file):\n",
    "        result = predictor.predict(audio_file)\n",
    "        print(\"\\nPrediction Result:\")\n",
    "        for k, v in result.items():\n",
    "            print(k, \":\", v)\n",
    "\n",
    "        visualize(audio_file, result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
